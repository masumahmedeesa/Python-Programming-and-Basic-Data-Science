### Hyperparameters and Model Validation in Machine Learning

---

### **1: Introduction to Hyperparameters**
- **What Are Hyperparameters?**
  - Hyperparameters are parameters that are **set before training** the model.
  - They control the learning process and affect the model’s performance but are not learned from the data.
  - **Reasons for hyperparameters**
    - Sometimes setting is chosen as a hyperparam because it is too difficult to optimize
    - More frequently, the setting is a hyperparam because it is not appropriate to learn that hyperparam on the training set
     - Applies to all hyperparameters for model capacity
       - **If learned on training set, they would always choose maximum model capacity resulting in overfitting**
  - Examples include:
    - Learning rate
    - Number of trees in a random forest
    - Number of layers in a neural network

---

### **2: Types of Parameters in Machine Learning**
- **1. Model Parameters**:
  - Learned during the training process.
  - Examples: weights in a neural network, coefficients in linear regression.
- **2. Hyperparameters**:
  - Set before training the model.
  - Not learned from the data but control how the model is trained.

---

### **3: Common Hyperparameters**
- **Learning Rate**:
  - Controls how much to change the model in response to errors each time the model weights are updated.
- **Number of Epochs**:
  - The number of times the learning algorithm will work through the entire training dataset.
- **Batch Size**:
  - The number of training examples used to estimate the error gradient in each step.
- **Number of Neurons** (for neural networks):
  - Controls the complexity of the model.
- **Regularization Parameter**:
  - Prevents overfitting by adding a penalty for larger weights.

---

### **4: Tuning Hyperparameters**
- **Why Tuning is Important**:
  - Hyperparameters significantly affect model performance.
  - Proper tuning can lead to a model that generalizes well to new, unseen data.
- **Methods to Tune Hyperparameters**:
  - **Grid Search**: Tries all possible combinations of hyperparameters.
  - **Random Search**: Samples random combinations of hyperparameters.
  - **Bayesian Optimization**: Uses probability models to find the best combination.

---

### **5: Introduction to Model Validation**
- **What is Model Validation?**
  - Model validation is the process of evaluating a model’s performance on a **separate dataset** that is not used for training.
  - Helps ensure the model generalizes well to unseen data.
  - Test examples should not be used to make choices about the model hyperparameters
  - **Training data is split into two disjoint parts**
    - First to learn the parameters
    - Other is the validation set to estimate generalization error during or after training
      - allowing for the hyperparameters to be updated
    - **Typically 80% of training data for training and 20% for validation**
  - **Test sets also need to change**
    - Over many years, the same test set used 
repeatedly to evaluate performance of different 
algorithms
    - With repeated attempts to beat state-of-the-art performance, we have optimistic evaluations with the test set as well
    - Community tends to move to new, usually more 
ambitious and larger benchmark data sets

---

### **6: Why Do We Need Model Validation?**
- **Avoid Overfitting**:
  - Overfitting happens when the model performs well on the training data but poorly on unseen data.
  - Validation helps prevent overfitting by testing the model on a separate validation set.
- **Estimate Model Performance**:
  - Helps assess the true performance of the model on new data.

---

### **7: Types of Validation Methods**
- **1. Hold-Out Validation**:
  - Split the dataset into **training** and **validation** sets.
  - Train the model on the training set and evaluate it on the validation set.
  - One disadvantage of using a holdout set for model validation is that we have **lost a portion** of our data to the model training
- **2. K-Fold Cross-Validation**:
  - The data is divided into **k** subsets.
  - The model is trained on **k-1** subsets and validated on the remaining subset. This process is repeated **k** times.
- **3. Leave-One-Out Cross-Validation (LOOCV)**:
  - Similar to k-fold, but each subset has only one data point. It is repeated for every data point in the dataset.
- **4. Stratified Cross-Validation**:
  - Ensures that each fold has a balanced representation of classes, which is useful for imbalanced datasets.

---

### **8: Steps of Model Validation Process**
1. **Step 1: Split Data**:
   - Separate your dataset into **training** and **validation** sets (or use cross-validation).
2. **Step 2: Train the Model**:
   - Train the model on the training set using predefined hyperparameters.
3. **Step 3: Validate the Model**:
   - Test the model on the validation set to check performance (e.g., accuracy, precision, recall).
4. **Step 4: Tune Hyperparameters**:
   - If necessary, adjust hyperparameters based on validation results.
5. **Step 5: Re-validate**:
   - After tuning, validate the model again to ensure improved performance.

---

### **9: Model Validation Metrics**
- **1. Accuracy**:
  - Percentage of correctly predicted instances.
- **2. Precision**:
  - The ratio of true positive predictions to all positive predictions.
- **3. Recall**:
  - The ratio of true positive predictions to all actual positives.
- **4. F1 Score**:
  - The harmonic mean of precision and recall.
- **5. AUC-ROC**:
  - Area under the ROC curve, measuring the model’s ability to distinguish between classes.

---

### **10: Final Model Testing**
- **After Validation:**
  - Once a model is tuned using hyperparameter optimization and validated, it is trained on the **entire training dataset**.
  - It is then tested on a **completely unseen test dataset** to evaluate final performance.

---

### **11: Conclusion**
- **Summary**:
  - Hyperparameters control the learning process and need careful tuning.
  - Model validation ensures that the model generalizes well to unseen data and avoids overfitting.
  - Proper validation and hyperparameter tuning are essential to building reliable machine learning models.

---

### **12: Q&A**
- **Questions?**

---

### Practise
[Colab](https://colab.research.google.com/drive/1_Bjqv2TTc1_mQVsDWJbjHIg2tP-dPDXL?usp=sharing)